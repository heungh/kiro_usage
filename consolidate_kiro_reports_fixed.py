#!/usr/bin/env python3
"""
Amazon Kiro User Activity Reports í†µí•© ìŠ¤í¬ë¦½íŠ¸ (ìˆ˜ì •ë¨)
"""

# ê³µí†µ ì„¤ì • import
from config import BUCKET_NAME, DEFAULT_REGION

import boto3
import pandas as pd
from datetime import datetime, timedelta
import argparse
from pathlib import Path
import io
import re


class KiroReportConsolidator:
    def __init__(self, bucket_name: str, base_prefix: str):
        self.s3 = boto3.client('s3')
        self.bucket_name = bucket_name
        self.base_prefix = base_prefix
    
    def extract_date_from_filename(self, filename: str) -> datetime:
        """íŒŒì¼ëª…ì—ì„œ ë‚ ì§œ ì¶”ì¶œ"""
        # íŒ¨í„´: {account_id}_by_user_analytic_202510200000_report.csv
        # ë‚ ì§œëŠ” ë§ˆì§€ë§‰ì—ì„œ ë‘ ë²ˆì§¸ ë¶€ë¶„ì˜ ì²« 8ìë¦¬
        match = re.search(r'_(\d{12})_', filename)
        if match:
            date_str = match.group(1)[:8]  # 202510200000 -> 20251020
            return datetime.strptime(date_str, '%Y%m%d')
        
        raise ValueError(f"ë‚ ì§œë¥¼ ì¶”ì¶œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {filename}")
    
    def list_csv_files(self, start_date: datetime = None, end_date: datetime = None):
        """S3ì—ì„œ CSV íŒŒì¼ ëª©ë¡ ì¡°íšŒ"""
        try:
            response = self.s3.list_objects_v2(
                Bucket=self.bucket_name,
                Prefix=self.base_prefix
            )
            
            csv_files = []
            for obj in response.get('Contents', []):
                key = obj['Key']
                if key.endswith('.csv'):
                    try:
                        filename = key.split('/')[-1]
                        file_date = self.extract_date_from_filename(filename)
                        
                        # ë‚ ì§œ í•„í„°ë§
                        if start_date and file_date < start_date:
                            continue
                        if end_date and file_date > end_date:
                            continue
                        
                        csv_files.append({
                            'key': key,
                            'date': file_date,
                            'size': obj['Size'],
                            'last_modified': obj['LastModified']
                        })
                    except ValueError:
                        continue
            
            return sorted(csv_files, key=lambda x: x['date'])
            
        except Exception as e:
            print(f"âŒ S3 íŒŒì¼ ëª©ë¡ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return []
    
    def download_and_parse_csv(self, key: str) -> pd.DataFrame:
        """S3ì—ì„œ CSV ë‹¤ìš´ë¡œë“œ ë° íŒŒì‹±"""
        try:
            response = self.s3.get_object(Bucket=self.bucket_name, Key=key)
            content = response['Body'].read()
            
            # CSV íŒŒì‹±
            df = pd.read_csv(io.BytesIO(content))
            
            # ë‚ ì§œ ì»¬ëŸ¼ ì¶”ê°€
            filename = key.split('/')[-1]
            file_date = self.extract_date_from_filename(filename)
            df['ReportDate'] = file_date.strftime('%Y-%m-%d')
            
            return df
            
        except Exception as e:
            print(f"âŒ CSV ë‹¤ìš´ë¡œë“œ/íŒŒì‹± ì‹¤íŒ¨ ({key}): {e}")
            return pd.DataFrame()
    
    def consolidate_reports(self, start_date: datetime = None, end_date: datetime = None) -> pd.DataFrame:
        """ëª¨ë“  ë¦¬í¬íŠ¸ë¥¼ í•˜ë‚˜ë¡œ í†µí•©"""
        print("ğŸ” S3ì—ì„œ CSV íŒŒì¼ ëª©ë¡ ì¡°íšŒ ì¤‘...")
        csv_files = self.list_csv_files(start_date, end_date)
        
        if not csv_files:
            print("âŒ ì¡°ê±´ì— ë§ëŠ” CSV íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return pd.DataFrame()
        
        print(f"âœ… {len(csv_files)}ê°œì˜ CSV íŒŒì¼ ë°œê²¬")
        
        all_dataframes = []
        
        for i, file_info in enumerate(csv_files, 1):
            key = file_info['key']
            date = file_info['date'].strftime('%Y-%m-%d')
            
            print(f"ğŸ“¥ [{i}/{len(csv_files)}] {date} ë°ì´í„° ë‹¤ìš´ë¡œë“œ ì¤‘...")
            
            df = self.download_and_parse_csv(key)
            if not df.empty:
                all_dataframes.append(df)
        
        if not all_dataframes:
            print("âŒ ìœ íš¨í•œ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return pd.DataFrame()
        
        # ëª¨ë“  DataFrame í•©ì¹˜ê¸°
        print("ğŸ”„ ë°ì´í„° í†µí•© ì¤‘...")
        consolidated_df = pd.concat(all_dataframes, ignore_index=True)
        
        # ì¤‘ë³µ ì œê±° (UserId + Date ê¸°ì¤€)
        before_count = len(consolidated_df)
        consolidated_df = consolidated_df.drop_duplicates(subset=['UserId', 'Date'], keep='last')
        after_count = len(consolidated_df)
        
        if before_count != after_count:
            print(f"âš ï¸  ì¤‘ë³µ ì œê±°: {before_count - after_count}ê°œ í–‰ ì œê±°ë¨")
        
        # ë‚ ì§œìˆœ ì •ë ¬
        consolidated_df = consolidated_df.sort_values(['ReportDate', 'UserId'])
        
        print(f"âœ… í†µí•© ì™„ë£Œ: {len(consolidated_df)}í–‰, {len(consolidated_df.columns)}ê°œ ì»¬ëŸ¼")
        
        return consolidated_df


def main():
    parser = argparse.ArgumentParser(
        description='Amazon Kiro User Activity Reports í†µí•©'
    )
    
    parser.add_argument(
        '--bucket',
        default=BUCKET_NAME,
        help='S3 ë²„í‚·ëª…'
    )
    
    parser.add_argument(
        '--prefix',
        default=f'daily-report/AWSLogs/{{account_id}}/KiroLogs/by_user_analytic/{DEFAULT_REGION}/',
        help='S3 í”„ë¦¬í”½ìŠ¤ (account_idëŠ” ìë™ìœ¼ë¡œ ëŒ€ì²´ë¨)'
    )
    
    parser.add_argument(
        '--start-date',
        help='ì‹œì‘ ë‚ ì§œ (YYYY-MM-DD)'
    )
    
    parser.add_argument(
        '--end-date',
        help='ì¢…ë£Œ ë‚ ì§œ (YYYY-MM-DD)'
    )
    
    parser.add_argument(
        '--output',
        default='consolidated_kiro_reports.csv',
        help='ì¶œë ¥ íŒŒì¼ëª…'
    )
    
    parser.add_argument(
        '--days',
        type=int,
        help='ìµœê·¼ Nì¼ ë°ì´í„°ë§Œ ì²˜ë¦¬'
    )
    
    args = parser.parse_args()
    
    # Account ID ìë™ ì¡°íšŒ ë° prefix ëŒ€ì²´
    if '{account_id}' in args.prefix:
        try:
            import boto3
            sts = boto3.client('sts')
            account_id = sts.get_caller_identity()['Account']
            args.prefix = args.prefix.replace('{account_id}', account_id)
            print(f"ğŸ” Account ID ìë™ ì¡°íšŒ: {account_id}")
        except Exception as e:
            print(f"âš ï¸ Account ID ì¡°íšŒ ì‹¤íŒ¨: {e}")
            print("ê¸°ë³¸ê°’ì„ ì‚¬ìš©í•˜ê±°ë‚˜ --prefixì—ì„œ {account_id}ë¥¼ ì‹¤ì œ ê°’ìœ¼ë¡œ ëŒ€ì²´í•˜ì„¸ìš”")
            return
    
    # data ë””ë ‰í† ë¦¬ ìƒì„±
    Path('data').mkdir(exist_ok=True)
    
    # ë‚ ì§œ ë²”ìœ„ ì„¤ì •
    start_date = None
    end_date = None
    
    if args.days:
        end_date = datetime.now()
        start_date = end_date - timedelta(days=args.days)
        print(f"ğŸ“… ìµœê·¼ {args.days}ì¼ ë°ì´í„° ì²˜ë¦¬: {start_date.strftime('%Y-%m-%d')} ~ {end_date.strftime('%Y-%m-%d')}")
    
    if args.start_date:
        start_date = datetime.strptime(args.start_date, '%Y-%m-%d')
    
    if args.end_date:
        end_date = datetime.strptime(args.end_date, '%Y-%m-%d')
    
    if start_date and end_date:
        print(f"ğŸ“… ë‚ ì§œ ë²”ìœ„: {start_date.strftime('%Y-%m-%d')} ~ {end_date.strftime('%Y-%m-%d')}")
    
    # í†µí•© ì‹¤í–‰
    consolidator = KiroReportConsolidator(args.bucket, args.prefix)
    
    print("="*80)
    print("ğŸ¯ Amazon Kiro Reports í†µí•© ì‹œì‘")
    print("="*80)
    
    consolidated_df = consolidator.consolidate_reports(start_date, end_date)
    
    if consolidated_df.empty:
        print("âŒ í†µí•©í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    # ê²°ê³¼ ì €ì¥
    output_path = Path(args.output)
    consolidated_df.to_csv(output_path, index=False)
    
    print("="*80)
    print("âœ… í†µí•© ì™„ë£Œ!")
    print("="*80)
    print(f"ğŸ“Š ê²°ê³¼:")
    print(f"  - ì´ í–‰ ìˆ˜: {len(consolidated_df):,}")
    print(f"  - ì´ ì‚¬ìš©ì ìˆ˜: {consolidated_df['UserId'].nunique():,}")
    print(f"  - ë‚ ì§œ ë²”ìœ„: {consolidated_df['ReportDate'].min()} ~ {consolidated_df['ReportDate'].max()}")
    print(f"  - ì¶œë ¥ íŒŒì¼: {output_path.absolute()}")
    
    # ê¸°ë³¸ í†µê³„
    print(f"\nğŸ“ˆ ê¸°ë³¸ í†µê³„:")
    if 'Chat_MessagesSent' in consolidated_df.columns:
        total_chat = consolidated_df['Chat_MessagesSent'].sum()
        print(f"  - ì´ Chat ë©”ì‹œì§€: {total_chat:,}")
    
    if 'Inline_SuggestionsCount' in consolidated_df.columns:
        total_inline = consolidated_df['Inline_SuggestionsCount'].sum()
        print(f"  - ì´ Inline ì œì•ˆ: {total_inline:,}")
    
    if 'Dev_GenerationEventCount' in consolidated_df.columns:
        total_dev = consolidated_df['Dev_GenerationEventCount'].sum()
        print(f"  - ì´ Dev ì´ë²¤íŠ¸: {total_dev:,}")
    
    print(f"\nğŸ’¡ ë‹¤ìŒ ë‹¨ê³„:")
    print(f"  - CSV í™•ì¸: head {output_path}")
    print(f"  - Excel ì—´ê¸°: open {output_path}")


if __name__ == "__main__":
    main()
